# DIP-SumEval: A Data Set of Human Summary Evaluations
This repository contains the summaries and evaluations from the paper _[A Dataset for the Analysis of Text Quality Dimensions in Summarization Evaluation](http://www.ke.tu-darmstadt.de/bibtex/publications/show/3200)_ presented at LREC 2020. 
In total, the data set contains 
* over 400 automatically generated summaries for 49 topics of an data set for multi-document summarization,
* 1274 judgements according to 11 text and summary quality criteria on a Likert-scale (1 to 5) performed by 26 trained annotators, 
* and 43218 pairwise judgements according to 6 criteria performed by 64 crowd-workers.

If you want to use the data, or just cite the paper, please use the following:
```
@INPROCEEDINGS{DIP-SumEval,
     author = {Mieskes, Margot and Loza Menc{\'{\i}}a, Eneldo and Kronsbein, Tim},
      title = {A Data Set for the Analysis of Text Quality Dimensions in Summarization Evaluation},
  booktitle = {Proceedings of the Twelfth International Conference on Language Resources and Evaluation (LREC 2020)},
       year = {2020},
       note = {To appear}
}
``` 
The data set is also accessible in the TUdatalib  institutional repository via https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/1904

## The Summaries
In the 'summaries' folder you find the summaries generated by ten different automatic systems. Each system summarized 49 topics, each containing a number of heterogenous documents. The topics are named 1001 to 1050, note that there is no topic 1009. The corpus used was presented by this paper: [Beyond Generic Summarization: A Multi-faceted Hierarchical Summarization Corpus of Large Heterogeneous Data](http://www.lrec-conf.org/proceedings/lrec2018/pdf/252.pdf)

## The Annotations
Find in the following some basic information about the annotations. For more details we refer to the descriptions in the paper.

### LikertAnno 
'LikertAnno.csv' contains annotations of the summaries of seven systems: H1-5, MMR and LeadFirst. A likert-scale of 1 to 5 is used, with 5 representing the best rating. The annotations jugde the summaries on eleven quality criteria: 
Non-Redundancy,
Referential Clarity,
Grammaticality,
Focus,
Structure,
Coherence,
Readability,
Information Content,
Spelling,
Length,
Overall Quality.

### PairAnnno
'PairAnnno.csv' contains the pairwise comparisons of the summaries of six systems: H1-4, MMR\*, PG-MMR and Submodular. The annotations were done by Amazon Mechanical Turk workers. Each annotation/row marks either the left summary or the right summary as better regarding the criterion (exactly 0 or 1).
Only the following criteria were used:
Non-Redundancy,
Referential Clarity,
Structure,
Readability,
Information Content,
Overall Quality.

# License
The annotations and summaries are licensed under the [Creative Commons CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.